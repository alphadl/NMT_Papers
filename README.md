# NMT_Papers (Update Continuously)

## Recommended Papers for NMT (Neural Machine Translation)

Some papers about NMT.

I have read all the papers. Hope you can fine something you need(like)!

### Model:

1. **Sequence to Sequence Learning with Neural Networks.**
*Ilya Sutskever, Oriol Vinyals, Quoc V. Le.* NIPS 2014. [paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

1. **Neural Machine Translation by Jointly Learning to Align and Translate.**
*Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.* ICLR 2015. [paper](https://arxiv.org/pdf/1409.0473.pdf)

1. **Effective Approaches to Attention-based Neural Machine Translation.**
*Minh-Thang Luong, Hieu Pham, Christopher D. Manning.* EMNLP 2015. [paper](http://aclweb.org/anthology/D15-1166)

1. **Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.**
*Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean.* 2016.  [paper](https://arxiv.org/pdf/1609.08144.pdf)

1. **Convolutional Sequence to Sequence Learning.**
*Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin.*  2017.  [paper](https://arxiv.org/pdf/1705.03122.pdf)

1. **Attention Is All You Need.**
*Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, Macduff Hughes.* ACL 2018.  [paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

1. **The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.**
*Sameen Maruf, Gholamreza Haffari.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1008)

1. **You May Not Need Attention.**
*Ofir Press, Noah A. Smith.*  2018.  [paper](https://arxiv.org/pdf/1810.13409.pdf)

### Analysis:

1. **Does String-Based Neural MT Learn Source Syntax?.**
*Xing Shi, Inkit Padhi, and Kevin Knight.* EMNLP 2016.  [paper](http://aclweb.org/anthology/D16-1159)

1. **What do Neural Machine Translation Models Learn about Morphology?**
*Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass.* ACL 2017.  [paper](https://arxiv.org/pdf/1704.03471.pdf)

1. **Massive Exploration of Neural Machine Translation Architectures.**
*Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le.* ACL 2017.  [paper](https://www.aclweb.org/anthology/D17-1151)

1. **What do Neural Machine Translation Models Learn about Morphology?**
*Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass.* ACL 2017.  [paper](https://arxiv.org/pdf/1704.03471.pdf)

1. **How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.**
*Tobias Domhan.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1167)

1. **Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.**
*Gongbo Tang, Mathias Müller, Annette Rios, Rico Sennrich.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1458)

1. **Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.**
*Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1396)

1. **The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.**
*Arianna Bisazza, Clara Tump.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1313)

1. **An Analysis of Encoder Representations in Transformer-Based Machine Translation.**
*Alessandro Raganato and Jorg Tiedemann.* EMNLP Worshopp BlackboxNLP 2018.  [paper](http://aclweb.org/anthology/W18-5431)

1. **When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?**
*Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Janani Padmanabhan, Graham Neubig.* NAACL 2018.  [paper](http://www.aclweb.org/anthology/N18-2084)

1. **On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference**
*Adam Poliak, Yonatan Belinkov, James Glass, Benjamin Van Durme.* NAACL 2018.  [paper](http://www.aclweb.org/anthology/N18-2082)

1. **A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation.**
*Surafel M. Lakew, Mauro Cettolo, Marcello Federico.* COLING 2018.  [paper](http://aclweb.org/anthology/C18-1054)

1. **On the Impact of Various Types of Noise on Neural Machine Translation.**
*Huda Khayrallah, Philipp Koehn.* WNMT 2018.  [paper](http://www.aclweb.org/anthology/W18-2709)

### Context Model:

1. **Neural Machine Translation with Extended Context.**
*Jorg Tiedemann and Yves Scherrer.* DiscoMT 2017.  [paper](http://www.aclweb.org/anthology/W17-4811)

1. **Does Neural Machine Translation Benefit from Larger Context?.**
*Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho.* EMNLP 2017.  [paper](https://arxiv.org/pdf/1704.05135.pdf)

1. **Exploiting Cross-Sentence Context for Neural Machine Translation.**
*Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu.* EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1301)

1. **Evaluating Discourse Phenomena in Neural Machine Translation.**
*Rachel Bawden, Rico Sennrich, Alexandra Birch, Barry Haddow.* NAACL 2018.  [paper](http://aclweb.org/anthology/N18-1118)

1. **Context-Aware Neural Machine Translation Learns Anaphora Resolution.** *Elena Voita, Pavel Serdyukov, Rico Sennrich, Ivan Titov.* ACL 2018.  [paper](http://www.aclweb.org/anthology/P18-1117)

1. **Exploiting Cross-Sentence Context for Neural Machine Translation.**
*Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu.* EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1301)

1. **Document Context Neural Machine Translation with Memory Networks.**
*Sameen Maruf, Gholamreza Haffari.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1118)

1. **Improving the Transformer Translation Model with Document-Level Context.**
*Jiacheng Zhang, Huanbo Luan, Maosong Sun, FeiFei Zhai, Jingfang Xu, Min Zhang, Yang Liu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1049)

1. **Document-Level Neural Machine Translation with Hierarchical Attention Networks.**
*Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1325)

1. **Handling Homographs in Neural Machine Translation.**
*Frederick Liu, Han Lu, Graham Neubig.* NAACL 2018.  [paper](http://aclweb.org/anthology/N18-1121)

1. **Improving the Transformer Translation Model with Document-Level Context.**
*Jiacheng Zhang, Huanbo Luan, Maosong Sun, FeiFei Zhai, Jingfang Xu, Min Zhang, Yang Liu.* EMNLP 2018. [paper](https://arxiv.org/pdf/1810.03581.pdf)


### Domain adaptation

1. **Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.**
*Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun Xie, Yongjing Yin, Jianqiang Zhao.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1041)

1. **Extreme Adaptation for Personalized Neural Machine Translation.**
*Paul Michel, Graham Neubig.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-2050)

1. **A Survey of Domain Adaptation for Neural Machine Translation.**  
*Chenhui Chu, Rui Wang.* COLING 2018.  [paper](https://arxiv.org/pdf/1806.00258.pdf)

1. **Regularization techniques for fine-tuning in neural machine translation.**
*Antonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann,  Rico Sennrich.  [paper](https://www.aclweb.org/anthology/D17-1156)

1. **Sentence Weighting for Neural Machine Translation Domain Adaptation.**
*Shiqi Zhang, Deyi Xiong.*COLING 2018. [paper](http://aclweb.org/anthology/C18-1269)

1. **Cost Weighting for Neural Machine Translation Domain Adaptation.**
*Boxing Chen, Colin Cherry, George Foster, Samuel Larkin.*WNMT2017 [paper](https://aclanthology.coli.uni-saarland.de/papers/W17-3205/w17-3205)

1. **Sentence selection and weighting for neural machine translation domain adaptation.**
*Rui Wang, Masao Utiyama, Andrew Finch, Lemao Liu, Kehai Chen, and Eiichiro Sumita.*I*EEE/ACM Transactions on Audio,Speech, and Language Processing.[paper](https://ieeexplore.ieee.org/document/8360031/)

1. **Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.**
*David Vilar.* NAACL 2018.  [paper](http://www.aclweb.org/anthology/N18-2080)

### Multi Task/Source NMT:
1. **Multi-Source Neural Translation.**
*Barret Zoph and Kevin Knight.* NAACL 2016.  [paper](http://www.aclweb.org/anthology/N16-1004)

1. **Multi-task Sequence to Sequence Learning.**
*Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser.* ICLR 2016.  [paper](https://arxiv.org/pdf/1511.06114.pdf)

1. **Attention Strategies for Multi-Source Sequence-to-Sequence Learning.**
*Jindřich Libovický, Jindřich Helcl.* ACL 2017.  [paper](http://aclweb.org/anthology/P17-2031)

1. **Input Combination Strategies for Multi-Source Transformer Decoder.**
*Jindřich Libovický, Jindřich Helcl and David Marecek.* WMT 2018.  [paper](http://www.statmt.org/wmt18/pdf/WMT026.pdf)


### Model Improvement :

 1. **Non-Autoregressive Neural Machine Translation.**
*Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, Richard Socher.* ICLR 2018.  [paper](https://openreview.net/pdf?id=B1l8BtlCb)

 1. **Semi-Autoregressive Neural Machine Translation.**
*Chunqi Wang, Ji Zhang, Haiqing Chen.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1044)

 #### Weight Tying

 1. **Using the Output Embedding to Improve Language Models.**
*Ofir Press, Lior Wolf.* EACL 2017.  [paper](http://aclweb.org/anthology/E17-2025)

 1. **Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation.**
*Nikolaos Pappas, Lesly Miculicich Werlen, James Henderson.* WMT 2018.  [paper](http://aclweb.org/anthology/W18-6308)

1. **Self-Attention with Relative Position Representations.**
*Peter Shaw, Jakob Uszkoreit, Ashish Vaswani.* ACL 2018.  [paper](http://aclweb.org/anthology/N18-2074)

 1. **Accelerating Neural Transformer via an Average Attention Network.**
*Biao Zhang, Deyi Xiong, Jinsong Su.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1166)

 ####Attention :

 ##### RNN seq2seq:

 1. **Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.**
*Junyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, Qi Su.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1331)

 1. **Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.**
*Shiv Shankar, Siddhant Garg, Sunita Sarawagi.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1065)

1. **Sparse and Constrained Attention for Neural Machine Translation.**
*Chaitanya Malaviya, Pedro Ferreira, André F. T. Martins.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-2059)

 1. **Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings.**
*Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, Deyi Xiong.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1164)

 ##### Transformer:

 1. **Modeling Localness for Self-Attention Networks.**
*Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, Tong Zhang.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1475)

 1. **On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.**
*Tamer Alkhouli, Gabriel Bretschner, Hermann Ney.* WMT 2018.  [paper](http://aclweb.org/anthology/W18-6318)

 1. **Multi-Head Attention with Disagreement Regularization.**
*Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, Tong Zhang.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1317)
 #### 

### Low Source:

1. **Rapid Adaptation of Neural Machine Translation to New Languages.**
*Graham Neubig, Junjie Hu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1103)

 1. **Copied Monolingual Data Improves Low-Resource Neural Machine Translation.**
*Anna Currey, Antonio Valerio Miceli Barone, and Kenneth Heafield.* WMT 2017.  [paper](http://www.aclweb.org/anthology/W17-4715)


### Multilingual NMT

1. **Contextual Parameter Generation for Universal Neural Machine Translation.**
*Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, Tom Mitchell.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1039)

 1. **Three Strategies to Improve One-to-Many Multilingual Translation.**
*Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu and Chengqing Zong.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1326)

### Unsupervised NMT:

### Incorporate Monolingual Data:

 1. **Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning.**
*Tobias Domhan and Felix Hieber.* EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1158)

 #### Back-Translation:

 1. **Improving Neural Machine Translation Models with Monolingual Data.**
*Rico Sennrich, Barry Haddow, Alexandra Birch.* ACL 2016.  [paper](http://www.aclweb.org/anthology/P16-1009)

 1. **Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.**
*Marzieh Fadaee, Christof Monz.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1040)

 1. **Understanding Back-Translation at Scale.**
*Sergey Edunov, Myle Ott, Michael Auli, David Grangier.* EMNLP 2018.  [paper](https://aclanthology.info/papers/D18-1045/d18-1045)

